{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5535c22-93b3-4d60-af16-6847123d8e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qdrant-client in /opt/anaconda3/lib/python3.11/site-packages (1.8.2)\n",
      "Collecting fastembed\n",
      "  Downloading fastembed-0.2.6-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /opt/anaconda3/lib/python3.11/site-packages (from qdrant-client) (1.62.1)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /opt/anaconda3/lib/python3.11/site-packages (from qdrant-client) (1.62.1)\n",
      "Requirement already satisfied: httpx>=0.20.0 in /opt/anaconda3/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/anaconda3/lib/python3.11/site-packages (from qdrant-client) (1.26.4)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from qdrant-client) (2.8.2)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in /opt/anaconda3/lib/python3.11/site-packages (from qdrant-client) (1.10.12)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /opt/anaconda3/lib/python3.11/site-packages (from qdrant-client) (2.0.7)\n",
      "Collecting huggingface-hub<0.21,>=0.20 (from fastembed)\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting loguru<0.8.0,>=0.7.2 (from fastembed)\n",
      "  Downloading loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting onnx<2.0.0,>=1.15.0 (from fastembed)\n",
      "  Downloading onnx-1.16.0-cp311-cp311-macosx_10_15_universal2.whl.metadata (16 kB)\n",
      "Requirement already satisfied: onnxruntime<2.0.0,>=1.17.0 in /opt/anaconda3/lib/python3.11/site-packages (from fastembed) (1.17.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.31 in /opt/anaconda3/lib/python3.11/site-packages (from fastembed) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.16.0,>=0.15.1 in /opt/anaconda3/lib/python3.11/site-packages (from fastembed) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.66 in /opt/anaconda3/lib/python3.11/site-packages (from fastembed) (4.66.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/anaconda3/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /opt/anaconda3/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client) (68.2.2)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.11/site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /opt/anaconda3/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<0.21,>=0.20->fastembed) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<0.21,>=0.20->fastembed) (2023.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<0.21,>=0.20->fastembed) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.11/site-packages (from huggingface-hub<0.21,>=0.20->fastembed) (23.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/lib/python3.11/site-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/lib/python3.11/site-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed) (24.3.25)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed) (1.12)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.31->fastembed) (2.0.4)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /opt/anaconda3/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /opt/anaconda3/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/lib/python3.11/site-packages (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->fastembed) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->fastembed) (1.3.0)\n",
      "Downloading fastembed-0.2.6-py3-none-any.whl (26 kB)\n",
      "Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnx-1.16.0-cp311-cp311-macosx_10_15_universal2.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: onnx, loguru, huggingface-hub, fastembed\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.22.1\n",
      "    Uninstalling huggingface-hub-0.22.1:\n",
      "      Successfully uninstalled huggingface-hub-0.22.1\n",
      "Successfully installed fastembed-0.2.6 huggingface-hub-0.20.3 loguru-0.7.2 onnx-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install qdrant-client fastembed openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16242e4d-4401-4520-b54d-e2087f0bc3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find image 'qdrant/qdrant:latest' locally\n",
      "latest: Pulling from qdrant/qdrant\n",
      "\n",
      "\u001b[1B764b1f6d: Pulling fs layer \n",
      "\u001b[1B1bbfc367: Pulling fs layer \n",
      "\u001b[1B1aa84de5: Pulling fs layer \n",
      "\u001b[1B87a9373b: Pulling fs layer \n",
      "\u001b[1B144233c8: Pulling fs layer \n",
      "\u001b[1B3d86a88d: Pulling fs layer \n",
      "\u001b[1B9e054cc5: Pulling fs layer \n",
      "\u001b[1Bb700ef54: Pull complete   32B/32B87MBB\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[3A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[2A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2KDigest: sha256:74d60f1d452520c8af3ac41a1e7e671ca7fb3b7569b8728af523136a2019a878\n",
      "Status: Downloaded newer image for qdrant/qdrant:latest\n",
      "6835aa02f32ce91c32a6503f06dfbc3eb0d2754f2be1e64b47dfbb6ab58023b8\n"
     ]
    }
   ],
   "source": [
    "!docker run -p \"6333:6333\" -p \"6334:6334\" --name \"rag-openai-qdrant\" --rm -d qdrant/qdrant:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2de0c9-3f50-4484-a1cb-ebc8675c4171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='test_collection'), CollectionDescription(name='news_embeddings'), CollectionDescription(name='better_news'), CollectionDescription(name='news_collection')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import qdrant_client\n",
    "\n",
    "client = qdrant_client.QdrantClient(\"http://localhost:6333\", prefer_grpc=True)\n",
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4092feda-161e-4df4-9c6c-b0df71c6070e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 77.7M/77.7M [00:05<00:00, 14.2MiB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['00dec2728ab74180b94dd5136df28a6f',\n",
       " '59e086d5674049d3bcbe2116b8c6a81e',\n",
       " 'e8fd07c001724476ba7f75e542b061d7',\n",
       " '8fa5a12839044de9b9714f0ae8da35b6',\n",
       " '7cebf4344ee44333a49d25fcac735169',\n",
       " '29a4698ffc734093bed798a869f99777',\n",
       " '745d3177ce0040f2a8462e713aee96b8',\n",
       " 'a87a8dfc7ee24b1bbf2e276e8d488a51']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.add(\n",
    "    collection_name=\"knowledge-base\",\n",
    "    documents=[\n",
    "        \"Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\",\n",
    "        \"Docker helps developers build, share, and run applications anywhere — without tedious environment configuration or management.\",\n",
    "        \"PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.\",\n",
    "        \"MySQL is an open-source relational database management system (RDBMS). A relational database organizes data into one or more data tables in which data may be related to each other; these relations help structure the data. SQL is a language that programmers use to create, modify and extract data from the relational database, as well as control user access to the database.\",\n",
    "        \"NGINX is a free, open-source, high-performance HTTP server and reverse proxy, as well as an IMAP/POP3 proxy server. NGINX is known for its high performance, stability, rich feature set, simple configuration, and low resource consumption.\",\n",
    "        \"FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\",\n",
    "        \"SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.\",\n",
    "        \"The cron command-line utility is a job scheduler on Unix-like operating systems. Users who set up and maintain software environments use cron to schedule jobs (commands or shell scripts), also known as cron jobs, to run periodically at fixed times, dates, or intervals.\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f473c7-3a99-49f7-b459-b49c57365778",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "What tools should I need to use to build a web service using vector embeddings for search?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33ffd890-feb9-4c51-93c1-c1cfb7f8e479",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.11/site-packages (0.21.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1528efdc-8a41-464b-93cb-32f97aa40994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8842d88-bf29-4e64-8a78-aea54b1ffdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To build a web service using vector embeddings for search, you will need the following tools:\n",
      "\n",
      "1. Programming language: Choose a programming language that supports machine learning and deep learning frameworks such as Python, Java, or C++. Python is commonly used for building web services with machine learning capabilities.\n",
      "\n",
      "2. Machine learning framework: Utilize machine learning frameworks such as TensorFlow, PyTorch, or Scikit-learn for creating and training the vector embeddings model.\n",
      "\n",
      "3. Web framework: Select a web framework such as Flask or Django for building the web service. These frameworks will help you handle requests, manage routes, and serve the model's predictions.\n",
      "\n",
      "4. Deployment platform: Choose a platform for deploying your web service, such as Docker or a cloud platform like AWS, Azure, or Google Cloud Platform.\n",
      "\n",
      "5. Database: Set up a database to store and retrieve the vector embeddings and associated metadata for efficient search operations. Options include SQL databases like PostgreSQL or NoSQL databases like MongoDB or Elasticsearch.\n",
      "\n",
      "6. Search engine: Incorporate a search engine like Elasticsearch to efficiently search and retrieve documents based on the vector embeddings.\n",
      "\n",
      "7. API documentation: Create clear and concise documentation for your API endpoints to help developers understand how to interact with your web service.\n",
      "\n",
      "By using these tools effectively, you can build a robust web service that leverages vector embeddings for powerful search capabilities.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    api_key=OPENAI_KEY\n",
    ")\n",
    "\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "069470cb-b78a-4fb2-b6a4-c6f523663bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[QueryResponse(id='00dec272-8ab7-4180-b94d-d5136df28a6f', embedding=None, sparse_embedding=None, metadata={'document': 'Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!'}, document='Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!', score=0.829069972038269),\n",
       " QueryResponse(id='29a4698f-fc73-4093-bed7-98a869f99777', embedding=None, sparse_embedding=None, metadata={'document': 'FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.'}, document='FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.', score=0.8190128803253174),\n",
       " QueryResponse(id='e8fd07c0-0172-4476-ba7f-75e542b061d7', embedding=None, sparse_embedding=None, metadata={'document': 'PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.'}, document='PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.', score=0.805652379989624)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = client.query(\n",
    "    collection_name=\"knowledge-base\",\n",
    "    query_text=prompt,\n",
    "    limit=3,\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70a7c144-e4c9-405b-a255-7220d60787d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\\nFastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\\nPyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\\n\".join(r.document for r in results)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e78c9a2-f657-486a-912f-7a4710d1ce47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a software architect. \n",
      "Answer the following question using the provided context. \n",
      "If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
      "\n",
      "Question: What tools should I need to use to build a web service using vector embeddings for search?\n",
      "\n",
      "Context: \n",
      "Qdrant is a vector database & vector similarity search engine. It deploys as an API service providing search for the nearest high-dimensional vectors. With Qdrant, embeddings or neural network encoders can be turned into full-fledged applications for matching, searching, recommending, and much more!\n",
      "FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
      "PyTorch is a machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing.\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metaprompt = f\"\"\"\n",
    "You are a software architect. \n",
    "Answer the following question using the provided context. \n",
    "If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
    "\n",
    "Question: {prompt.strip()}\n",
    "\n",
    "Context: \n",
    "{context.strip()}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Look at the full metaprompt\n",
    "print(metaprompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba7b9263-09fb-4553-bf54-f4efade34030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-99d53yEz7xEoAWvHixSHbvJYtWdot', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='You would need Qdrant for the vector database and similarity search engine, FastAPI for building the API service, and PyTorch for working with embeddings or neural network encoders.', role='assistant', function_call=None, tool_calls=None))], created=1712082205, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint='fp_b28b39ffa8', usage=CompletionUsage(completion_tokens=37, prompt_tokens=188, total_tokens=225))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_new = openai_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": metaprompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "completion_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cb32354-0a8e-4927-9a91-b6396dd41838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You would need Qdrant for the vector database and similarity search engine, FastAPI for building the API service, and PyTorch for working with embeddings or neural network encoders.\n"
     ]
    }
   ],
   "source": [
    "print(completion_new.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcd987fa-5064-44de-9bbf-0723f5202de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(question: str, n_points: int = 3) -> str:\n",
    "    results = client.query(\n",
    "        collection_name=\"knowledge-base\",\n",
    "        query_text=question,\n",
    "        limit=n_points,\n",
    "    )\n",
    "\n",
    "    context = \"\\n\".join(r.document for r in results)\n",
    "\n",
    "    metaprompt = f\"\"\"\n",
    "    You are a software architect. \n",
    "    Answer the following question using the provided context. \n",
    "    If you can't find the answer, do not pretend you know it, but answer \"I don't know\".\n",
    "    \n",
    "    Question: {question.strip()}\n",
    "    \n",
    "    Context: \n",
    "    {context.strip()}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    completion_new = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": metaprompt}\n",
    "        ],\n",
    "        timeout=10.0,\n",
    "    )\n",
    "    \n",
    "    return completion_new.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9af6e7a4-007e-47c0-ad8a-60a41f0619f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The stack for a web API using FastAPI, Docker, and NGINX could look like this:\\n1. FastAPI for building the API endpoints and handling HTTP requests/responses.\\n2. Docker to containerize the FastAPI application for easy deployment and management.\\n3. NGINX as a reverse proxy server to handle incoming client requests, load balancing, and potentially serving static content.\\nOverall, this stack would provide a high-performance, scalable, and reliable environment for hosting a web API.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"What can the stack for a web api look like?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2549a71c-318f-43f4-b6ec-5688d397b2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"Where is the nearest grocery store?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b3c183d-cd07-4e11-a2fd-70925df961da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag-openai-qdrant\n",
      "Error response from daemon: No such container: rag-openai-qdrant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!docker kill rag-openai-qdrant\n",
    "!docker rm rag-openai-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa6989-fd9a-4c65-aab4-d6417cc8db90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
